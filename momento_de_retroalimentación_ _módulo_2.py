# -*- coding: utf-8 -*-
"""Momento de Retroalimentación | Módulo 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G4mMXPSmMDvSFa7Br0squJblFuO-5DL1

# Momento de Retroalimentación
#### *Módulo 2*
---
David Emiliano Mireles Cárdenas

## Introducción

El Dataset *Statlog (German Credit Data)* fue donado a la Universidad de California en 1994 por el profesor Hans Hoffman. Contiene mil filas y 21 columnas, veinte de ellas corresponden a atributos y la última corresponde a la columna *target*.

El objetivo de este dataset es predecir la última columna, que determina si el cliente de un banco es un buen o mal candidato para obtener un crédito. En este caso en particular es preferible catalogar a alguien aceptable para recibir un crédito como inaceptable, en lugar de catalogar a alguien inaceptable como aceptable. Es decir, el banco prefiere rechazar una soliticud de crédito a alguien que sería bueno manejandolos sobre otorgar el crédito a alguien que sería malo manejandolo. Es por esto que este dataset viene con una matriz de costo que se pide utilizar:

![cost_matrix.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA0AAAAB+CAYAAAAN+3ECAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAmjSURBVHhe7d3fa5RXHgfgr/s3OMSytS5JyEWWppIUelGw60popJcppZZlwZAFCy2UgrYpthddUatQCi0oVPSmaBFzudQluK2BXghryKY0F8GEtXbZZCf/gzvv5GQyk0SjNdnNzHkeeMk57xlyN4fzOb9mx/2KAAAAyMCv0l8AAICWJwABAADZEIAAAIBsCEAAAEA2BCAAACAbAhAAAJANAQhoXj9+He98ejMWUhVgu1n42+fxzuXpVAO2A78DBGyi6bg48nVMpVrVs6/HZ290p8omKwLQX5+KkXdfirb0CuBJFIHl1Fg51ZIn6Meq/2/+wNb1g8BjswIEbI4ijIzciN3vfhyfnap7npuKiz+mzwA0gyLw1Pqx16Pnh6/1Y9BCrAABm6AcY59+Hvde/jiGfptePVDjKlFb/9sx8vtSqhUe3l4/O9tWKsVC9FgBAjbN2hWbok8qJnfejv7lrqiY8PlqeVtbdwwVISnVClOXP4qLPyyVq/3ULitAsJ0IQMCTK9+MU59OxfP1A4R1rR5IrA5OG7Sv2vJWHahMCkDA5lm7BW51wKn0S5eno+eNun6oLjCtrlfDUGzhVmDgsdkCB2yNIhSNfBTvVJ7a1pFyORZKPdFTC0ml6NlbioWFNNjYoL3427a3uxZ22toemrYAfpmGLXA98ff6fqzSL/Wn8FNY3Q8tzJej57mVsNO2Sz8F240ABDy5Unc8XyrHvf+keqH0UoxUBg9Dz6Y6QFPqjlf6SzH1j7qb3OomeFa2wgHNQgACNkEp+l/ujqmvPo/Vlyc1KIJSTMVU7TPlmJosr8ygbtBe/F2YnK5de11bOQLYMkv9UG1Vp7rl99/xyvIK0R8at7YVKz71YalYEQK2F2eAgE3UeIFBVenAqjM6D7/kYKP2hsPFz1YGHvNPxZAzQMAmWXsGaHU/tHQ28S/LHymVKqGn0s/Vzvg0tvdU+qmp6HEGCLYRAQgAAMiGLXAAAEA2BCAAACAbAhAAAJANAQgAAMiGAAQAAGRDAAIAALIhAAEAANkQgAAAgGwIQAAAQDYEIAAAIBsCEAAAkA0BCAAAyIYABAAAZEMAAgAAsiEAAQAA2RCAAACAbAhAAABANgQgAAAgGwIQAACQjR33K1K56qeffkolAACA5vfMM8+k0joBCAAAoFXZAgcAAGRDAAIAALIhAAEAANkQgAAAgGwIQAAAQDYEIAAAIBsCEAAAkA0BCAAAyIYABAAAZEMAAgAAsiEAAQAA2RCAAACAbAhAAABANgQgAAAgGwIQAACQjQcHoPnRGOrqjva65+Tt1LYVbp/d2v8PbLny6JFafzE0upjeAjSZypikvetIXJ1PdaClrBuAqoOY4xGfzEzHXN0zEJPpEwCrVAYML8weqfUXb83uM6kBNJnJOFlM4tzoj1unOtI7oNWsDUDzo/FeMYi5MBil9GpZb9/eVAJoNHEj4tqxlT6i99jliBsmTYBmsjc+KCZx6voyoPWsCUDl7+di4I+P8sVPsyTLz5nVA50N2qvLy8vtZ2MivQaa0WRcj/7oTbUlT0dnzEY51QAAtoM1AehetMf+XanyQEW4GYuBtNWleG51nK/b879BexF+bvTX2uZm+uP6oUtLbUCL2BmddpAAANvMgy9BWFZ/GcLwaHU2tzx6PuLK0YbZ3tLgiRj45lJ1Jefh7Ytx9VzjVpnqkvOVw6kMAACwNdYGoNm5uJeKVbsG42J1lWY8Tneld9ERnb9OxZqd0fmo7V3tsTsVgVa1GHdmUxEAYJtYE4B6D0R88Yuur60MdmZScV0Pby//00gJmtfT0Tkz1niWb/5mXI+ONZepAAD8P61dAeo7Wr2+du2lBitKg/1xZ1/jxQXl0eNx/eDh6ra3h7fvjP0dY/Fefcgqbp4bGU8VoPlUvtcHZ+PVWr+xGFePjz3ihSoAAP87O+5XpHKD4reAXmgIJfvi9Pj5eG35goTibNC+D+O7VI3hy43XRm7QPnGmO169kCr7/hy33pyLL+NofNCX3gFNp/57PXxl2vcZaFrl0bPx7YtHV8Y9QMt4YAACAABoNRvfAgcAANAiBCAAACAbAhAAAJANAQgAAMiGAAQAAGRDAAIAALIhAAEAANkQgAAAgGwIQAAAQDYEIAAAIBsCEAAAkA0BCAAAyIYABAAAZEMAAgAAsrHjfkUqV929ezeVAAAAmt+ePXtSaZ0ABAAA0KpsgQMAALIhAAEAANkQgAAAgGwIQAAAQDYEIAAAIBsCEAAAkA0BCAAAyIYABAAAZEMAAgAAsiEAAQAA2RCAAACAbAhAAABANgQgAAAgGwIQAACQDQEIAADIxtoAdPtstHd11z1nYyI1Pb7FuHpmNMqpBrSyyTjZ0HcciavzqQmgmdSPhc5MppdAq1h3BWj4ynTMzaRnvD2+8OUHHkHnqfGVvmPmfLy2KzUANIv50Rg61x63Ul92q+N8DI0upkagFWy8BW7XSzEQs1ZxgIebn407qQjQrMrfz8XAicEopXpp8EQMzN40DoIWsnEAun0p7hxY6Qgat8it3uKyGFeHl9u6Y2j05/QeyEHnb3amEkAzWoxvZ9tjf8Pq9c7ojLm4l2pA81s3AF04tBJi2s+1x5/6UkPFRPSvbHEZ74/rx5fP+BTh53jEieXtL9PxSZyP92eqjUCr+9dcvF/Xd5y8nd4DNLndHe2pBLSCjc8AnYh4b3jlIoPevr2pVLFrMN7qSrMity/F9YMnGvb8F8vGp7tSBWhtfUdX+o2Z8eg85xIEAGD7eYQzQIPxycG5+LY2kGm86enVC+l1he0vwJKd8dqJ/rjzvYPDQPO7NzuXSkAr2DgA1StuRukai4HaLO90XBtObev6Oe7YAgcANIXivM9Y3aRvYTKuz7TH7lQDmt8jBKDJ+PKblQOBnacOR+9SsXohQm0FqK8/4lDjbwZNnHkj6haIgGwsxtXjY9H5olVhoLn0HuiI92vnm5fGMvFm3WVQQNPbcb8ilZcUt7wdupQqhcNxbeZoLfRMnKnb9jZ8Oa7FWMSx1F6sEO37ML6rNhZnicaj88bN2H9MxwGtrqFviH1xetzvAAHNqTx6JF4YGa+Wf3dqPC4OmsyBVrI2AAEAALSoxzsDBAAA0MQEIAAAIBsCEAAAkA0BCAAAyIYABAAAZEMAAgAAsiEAAQAA2RCAAACAbAhAAABANgQgAAAgGwIQAACQDQEIAADIhgAEAABkQwACAACyseN+RSpX3b17N5UAAACa3549e1JpnQAEAADQqmyBAwAAsiEAAQAA2RCAAACAbAhAAABAJiL+C7qoKXUv7NO6AAAAAElFTkSuQmCC)

La fuente es el [sitio de Machine Learning de UC Irvine](http://archive.ics.uci.edu/dataset/144/statlog+german+credit+data).

## Preparación

### Importar librerías necesarias
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import RobustScaler
from sklearn.feature_selection import f_regression, SelectKBest
from sklearn.model_selection import KFold, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from imblearn.ensemble import EasyEnsembleClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix

"""### Cargar datos

Conectar con Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""Importar datos"""

df = pd.read_csv('/content/drive/MyDrive/Statlog German Credit Data/german.data',
                 header = None, # no hay header row
                 sep = ' ') # se usan espacios en lugar de comas

"""A continuación la descripción de los datos obtenida del sitio oficial de UC Irvine.

>**Attribute 1: (qualitative)**  
***Status of existing checking account***  
- **A11:** ... <    0 DM
- **A12:** 0 <= ... <  200 DM
- **A13:** ... >= 200 DM / salary assignments for at least 1 year
- **A14:** no checking account
>
>**Attribute 2: (numerical)**  
***Duration in month***
>
>**Attribute 3: (qualitative)**  
***Credit history***  
- **A30:** no credits taken/ all credits paid back duly
- **A31:** all credits at this bank paid back duly
- **A32:** existing credits paid back duly till now
- **A33:** delay in paying off in the past
- **A34:** critical account/ other credits existing (not at this bank)
>
>**Attribute 4: (qualitative)**  
***Purpose***  
- **A40:** car (new)
- **A41:** car (used)
- **A42:** furniture/equipment
- **A43:** radio/television
- **A44:** domestic appliances
- **A45:** repairs
- **A46:** education
- **A47:** (vacation - does not exist?)
- **A48:** retraining
- **A49:** business
- **A410:** others
>
>**Attribute 5: (numerical)**  
***Credit amount***
>
>**Attribute 6: (qualitative)**  
***Savings account/bonds***  
- **A61:** ... <  100 DM
- **A62:** 100 <= ... <  500 DM
- **A63:** 500 <= ... < 1000 DM
- **A64:** ... >= 1000 DM
- **A65:** unknown/ no savings account
>
>**Attribute 7: (qualitative)**  
***Present employment since***  
- **A71:** unemployed
- **A72:** ... < 1 year
- **A73:** 1  <= ... < 4 years  
- **A74:** 4  <= ... < 7 years
- **A75:** ... >= 7 years
>
>**Attribute 8: (numerical)**  
***Installment rate in percentage of disposable income***
>
>**Attribute 9: (qualitative)**  
***Personal status and sex***  
- **A91:** male   : divorced/separated
- **A92:** female : divorced/separated/married
- **A93:** male   : single
- **A94:** male   : married/widowed
- **A95:** female : single
>
>**Attribute 10: (qualitative)**  
***Other debtors / guarantors***  
- **A101:** none
- **A102:** co-applicant
- **A103:** guarantor
>
>**Attribute 11: (numerical)**  
***Present residence since***
>
>**Attribute 12: (qualitative)**  
***Property***  
- **A121:** real estate
- **A122:** if not A121 : building society savings agreement/ life insurance
- **A123:** if not A121/A122 : car or other, not in attribute 6
- **A124:** unknown / no property
>
>**Attribute 13: (numerical)**  
***Age in years***
>
>**Attribute 14: (qualitative)**  
***Other installment plans***  
- **A141:** bank
- **A142:** stores
- **A143:** none
>
>**Attribute 15: (qualitative)**  
***Housing***  
- **A151:** rent
- **A152:** own
- **A153:** for free
>
>**Attribute 16: (numerical)**  
***Number of existing credits at this bank***
>
>**Attribute 17: (qualitative)**  
***Job***  
- **A171:** unemployed/ unskilled  - non-resident
- **A172:** unskilled - resident
- **A173:** skilled employee / official
- **A174:** management/ self-employed/
  highly qualified employee/ officer
>
>**Attribute 18: (numerical)**  
***Number of people being liable to provide maintenance for***
>
>**Attribute 19: (qualitative)**  
***Telephone***  
- **A191:** none
- **A192:** yes, registered under the customer's name
>
>**Attribute 20: (qualitative)**  
***Foreign worker***  
- **A201:** yes
- **A202:** no

### Preprocesamiento de datos / Data Preprocessing inicial

Realizamos parte del preprocesamiento. Omitimos por ahora el *encoding*, y escalamiento/normalización para explorar los datos con nombres de categorías descriptivos y valores reales más adelante. Obtenemos un vistazo inicial de los datos.
"""

df.head()

"""Evidentemente, los datos importados no son muy entendibles. Primeramente no contienen nombres para las columnas. Las añadimos a continuación."""

df.columns = [
    'Checking_Status',
    'Duration',
    'Credit_History',
    'Purpose',
    'Credit_Amount',
    'Savings_Account',
    'Employment_Since',
    'Installment_Rate',
    'Personal_Status',
    'Debtors_Guarantors',
    'Residence_Since',
    'Property_Type',
    'Age',
    'Other_Plans',
    'Housing',
    'Num_Credits_At_Bank',
    'Job_Type',
    'Num_People_Liable',
    'Telephone',
    'Foreign_Worker',
    'Classification'
]

"""Adicionalmente, el *encoding* de los datos es realizado de una forma extraña. Todos los valores categóricos empiezan con la letra A, seguido del número de columna al que corresponden y terminan con el número de categoría. Por ejemplo, el valor

> A11

quiere decir que es la primera categoría de la columna 1. Asimismo, el valor

> A12

quiere decir que es la segunda categoría de la columna 1. Cambiamos esto temporalmente por texto (será cabmiado por encoding después) para entender los datos mejor en la exploración.

Creamos estructuras de datos que contienen nombres más descriptivos para las categorías.
"""

checking_status_mapping = {
    'A11': '< 0',
    'A12': '0-200',
    'A13': '>= 200',
    'A14': 'No Account'
}

credit_history_mapping = {
    'A30': 'No Credits Outstanding', # No debe nada
    'A31': 'No Credits Outstanding Here', # No debe nada en este banco
    'A32': 'On Time Payment for Outstanding Credits', # Tiene deudas pero ha pagado a tiempo hasta ahora
    'A33': 'Delay in Paying', # Pagos a destiempo, falta de puntualidad
    'A34': 'Critical'
}

purpose_mapping = {
    'A40': 'Car (New)',
    'A41': 'Car (Used)',
    'A42': 'Furniture/Equipment',
    'A43': 'Radio/TV',
    'A44': 'Domestic Appliances',
    'A45': 'Repairs',
    'A46': 'Education',
    'A47': 'Vacation',
    'A48': 'Retraining',
    'A49': 'Business',
    'A410': 'Others'
}

savings_account_mapping = {
    'A61': '< 100',
    'A62': '100-500',
    'A63': '500-1000',
    'A64': '>= 1000',
    'A65': 'Unknown/None'
}

employment_since_mapping = {
    'A71': 'Unemployed',
    'A72': '< 1 Year',
    'A73': '1-4 Years',
    'A74': '4-7 Years',
    'A75': '>= 7 Years'
}

personal_status_mapping = {
    'A91': 'Male: Divorced/Separated',
    'A92': 'Female: Divorced/Separated/Married',
    'A93': 'Male: Single',
    'A94': 'Male: Married/Widowed',
    'A95': 'Female: Single'
}

debtors_guarantors_mapping = {
    'A101': 'None',
    'A102': 'Co-Applicant',
    'A103': 'Guarantor'
}

property_mapping = {
    'A121': 'Real Estate',
    'A122': 'Building Society/Insurance',
    'A123': 'Car or Other',
    'A124': 'Unknown/No Property'
}

other_plans_mapping = {
    'A141': 'Bank',
    'A142': 'Stores',
    'A143': 'None'
}

housing_mapping = {
    'A151': 'Rent',
    'A152': 'Own',
    'A153': 'Free'
}

job_mapping = {
    'A171': 'Unemployed - Non-Resident',
    'A172': 'Unskilled - Resident',
    'A173': 'Skilled Employee/Official',
    'A174': 'Management/Self-Employed'
}

telephone_mapping = {
    'A191': 'None',
    'A192': 'Yes'
}

foreign_worker_mapping = {
    'A201': 'Yes',
    'A202': 'No'
}

"""Sustituímos los nombres actuales por los que acabamos de crear."""

df['Checking_Status'] = df['Checking_Status'].replace(checking_status_mapping)
df['Credit_History'] = df['Credit_History'].replace(credit_history_mapping)
df['Purpose'] = df['Purpose'].replace(purpose_mapping)
df['Savings_Account'] = df['Savings_Account'].replace(savings_account_mapping)
df['Employment_Since'] = df['Employment_Since'].replace(employment_since_mapping)
df['Personal_Status'] = df['Personal_Status'].replace(personal_status_mapping)
df['Debtors_Guarantors'] = df['Debtors_Guarantors'].replace(debtors_guarantors_mapping)
df['Property_Type'] = df['Property_Type'].replace(property_mapping)
df['Other_Plans'] = df['Other_Plans'].replace(other_plans_mapping)
df['Housing'] = df['Housing'].replace(housing_mapping)
df['Job_Type'] = df['Job_Type'].replace(job_mapping)
df['Telephone'] = df['Telephone'].replace(telephone_mapping)
df['Foreign_Worker'] = df['Foreign_Worker'].replace(foreign_worker_mapping)

df.head().T

"""Tenemos un vistazo de los datos con los nuevos nombres de columnas y valores entendibles.

A continuación verificamos que no hayan datos nulos.
"""

df.isna().sum()

"""Afortunadamente no existen datos nulos."""

df.dtypes

"""### Exploración de datos

Para los datos numéricos creamos histogramas:
"""

numeric_columns = [
    'Duration',
    'Credit_Amount',
    'Installment_Rate',
    'Residence_Since',
    'Age',
    'Num_Credits_At_Bank',
    'Num_People_Liable'
]
sns.set(style="whitegrid")

fig, axes = plt.subplots(nrows=len(numeric_columns), ncols=1, figsize=(8, 6 * len(numeric_columns)))

for i, col in enumerate(numeric_columns):
    sns.histplot(data=df, x=col, kde=True, ax=axes[i])
    axes[i].set_title(f'Histograma de {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frecuencia')

plt.tight_layout()
plt.show()

"""Podemos observar:
* La mayoría de los créditos son por 12 o 24 meses. Existe una predisposición para los créditos de menos plazo aunque si los hay de plazos más largo (pero la distribución esta orientada más al lado izquierdo).
* El crédito promedio es por un monto de menos de 2,5000 DM (alrededor de 1,025 dólares).
* La distribución de las tasas de interés está inclinada hacia la derecha, con la mayoría de los registtros en 4%
* La mayoría de las personas tienen 4 años de residentes.
* La distribución para la edad, al igual que para las dos anteriores variables, esta inclinada hacia la izquierda. La media esta entre 20 y 30 años.
* La mayoría de las personas tienen uno o dos créditos en el banco (mucho más común tener uno solo).
* La mayoría de los casos tienen una sola persona responsable de los pagos.

Para los datos categóricos creamos gráficos de barras:
"""

categorical_columns = [
    'Checking_Status',
    'Credit_History',
    'Purpose',
    'Savings_Account',
    'Employment_Since',
    'Personal_Status',
    'Debtors_Guarantors',
    'Property_Type',
    'Other_Plans',
    'Housing',
    'Job_Type',
    'Telephone',
    'Foreign_Worker'
]

sns.set(style="whitegrid")

fig, axes = plt.subplots(nrows=len(categorical_columns),
                         ncols=1,
                         figsize=(10, 6 * len(categorical_columns)))

for i, col in enumerate(categorical_columns):
    sns.countplot(data=df, x=col, ax=axes[i])
    axes[i].set_title(f'Frecuencias de {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frecuencia')

for ax in axes:
    ax.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

"""Obtengamos las estadísticas descriptivas"""

df.describe().T

sns.set(style="whitegrid")
sns.set_context("notebook")

for column in numeric_columns:
    plt.figure(figsize=(8, 4))
    sns.boxplot(data=df, x=column)
    plt.title(f'Box Plot of {column}')
    plt.xlabel(column)
    plt.show()

"""Observamos que las columnas numéricas no tienen muchos outliers que realmente afectarían. Adicionalmente, estos son valores probablemente reales. Sin embargo, quizás los outliers puedan afectar el ecalamiento. Utilizaremos *RobustScaler* en lugar de *StandardScaler* más adelante para minimizar el efecto de outliers.

Observemos la distribución de clasificación recordando que 1 significa bueno y 2 significa malo. Primero cambiaremos que 0 signifique malo para tener mayor claridad.
"""

df['Classification'] = df['Classification'].replace(2, 0)

class_counts = df['Classification'].value_counts()

plt.figure(figsize=(6, 6))
sns.set(style="whitegrid")
plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribución de Classification')
plt.axis('equal')

plt.show()

"""Podemos ver que el dataset está desbalanceado, habiendo más del doble de filas con 1 en classification que 0. Es importante considerar esto para el entrenamiento.

Observaciones:
* Para el estado de la cuenta de cheques, lo más común es no tener, quizás esta variable no sea tan importante.
* Los dos casos más comunes de historial crediticio son pagos a tiempo en créditos actuales y estados de cuenta críticos (el peor caso posible).
* La mayoría de los créditos son para automoviles, TV/radios o muebles, lo cuál hace sentido con los montos de los créditos.
* La mayoría de las personas tienen entre 0 y 100 DM en su cuenta de ahorros. Considerando que los créditos son en promedio por 1,000 DM, quizás esta columna no sea indicativa de buenos deudores.
* El estatus de empleo más común es empleos con 1-4 años de antiguedad. El menos común es estar desempleado. Quizás sea útil crear una columna de empleado/desempleado en lugar de esta.
* La mayoría de las personas son hombres solteros. En segundo lugar son mujeres. Esta columna no parece ser muy útil.
* La gran mayoría de las personas no tienen a alguien como aval de su crédito. Quizás esto no sea indicativo de un buen o mal crédito.
* Muchas personas no tienen propiedad. El tipo de propiedad más común es un automóvil. En segundo lugar existen propiedades de bienes raíces. También es posible convertir esta columna a encoding binario (tiene o no tiene propiedad).
* La mayoría de las personas no tienen otros planes con mensualidades, ni en tiendas ni bancos.
* La mayoría de las personas son dueñas de su propio hogar.
* La mayoría de las personas son *Skilled Employee / Official*. Probablemente es más útil convertir esta columna en tiene/no tiene trabajo.
* No se tiene teléfono para la mayoría de las personas. La columna es de poca utilidad.
* La mayoría de las personas son trabajadores extranjeros. El porcentaje es tanto que probablemente no sea útil contar con esta columna.
* La mayoría (70%) de los casos corresponden a una buena clasificacion.

### Preprocesamiento de datos / Data Preprocessing

* Checking_Status: Cambiaremos por encoding label/ordinal ya que los incrementos son por la misma cantidad (200 DM)
* Credit_History: Cambiaremos por encoding binario (bueno = 1, malo = 0)
* Purpose: Cambiaremos por encoding binario (si es para educación o negocio = 1, si es para otra cosa = 0)
* Savings_Account: Eliminaremos la columna
* Employment_Since: Encoding binario (empleado = 1, desempleado = 0)
* Personal_Status: Eliminaremos la columna
* Debtors_Guarantors: Eliminaremos la columna
* Property_Type: Encoding binario (tiene propiedad = 1, no tiene propiedad = 0)
* Other_Plans: Eliminaremos la columna
* Housing: Cambiaremos por encoding binario (es dueño de su casa = 1, no es dueño de su casa = 0)
* Job_Type: *Cambiaremos la columna por encoding*
* Telephone: Eliminaremos la columna
* Foreign_Worker: Eliminaremos la columna
"""

drop_cols = [
    'Savings_Account',
    'Personal_Status',
    'Debtors_Guarantors',
    'Other_Plans',
    'Telephone',
    'Foreign_Worker'
]

df.drop(drop_cols, axis = 1, inplace = True)

"""#### Encoding

Lidiar con columnas categóricas presenta una serie de decisiones necesarias, principalmente del *encoding* o codificación de los datos. Esto es aún más necesario cuando se considera el formato en que las columnas categóricas están.

Es necesario elegir un método de *encoding* para cada columna categórica, basado principalmente en si una columna es ordinal o nominal, además del modelo de aprendizaje automático que se usará.
"""

# Checking_Status
df['Checking'] = [0 if x == 'No Account'
                 else 1 if x == '< 0'
                 else 2 if x == '0-200'
                 else 3
                 for x in df['Checking_Status']]

# Credit History
df['Good_History'] = [1 if x == 'On Time Payment for Outstanding Credits'
                      or x == 'No Credits Outstanding'
                      or x == 'No Credits Outstanding Here'
                      else 0
                      for x in df['Credit_History']]

# Purpose
df['Business_or_Education'] = [1 if x == 'Business'
                               or x == 'Education'
                               else 0
                               for x in df['Purpose']]

# Employment_Since
df['Unemployed'] = [1 if x == 'Unemployed'
                   else 0 for x in df['Employment_Since']]

# Propety_Type
df['Property_Owner'] = [1 if x != 'Unknown/No Property'
                   else 0 for x in df['Property_Type']]
# Housing
df['Owns_House'] = [1 if x == 'Own'
                   else 0 for x in df['Housing']]

# Job_Type
df = pd.get_dummies(df, columns = ['Job_Type'])

"""Eliminamos los originales de las nuevas columnas que acabamos de crear."""

drop_cols = ['Checking_Status',
             'Credit_History',
             'Purpose',
             'Employment_Since',
             'Property_Type',
             'Housing'] # get_dummies() ya eliminó Job_Type

df.drop(drop_cols, axis = 1, inplace = True)

"""#### Escalamiento

Escalaremos los datos para hacer que
"""

scaler = RobustScaler()
df[numeric_columns] = scaler.fit_transform(df[numeric_columns])

"""Observemos los datos escalados."""

df.head()

"""### Exploración de datos: Análisis de correlación"""

correlation_matrix = df.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

"""Correlaciones notables:
* Credit amount y duration: Esto hace sentido ya que entre mayor sea el monto del crédito es más probable que el deudor desee plazos más largos para sus pagos.
* Good credit history y amount of credits at bank: Esto hace sentido ya que entre más cantidad de créditos mayor la oportunidad de no pagar a tiempo.
* Para todas las columnas de Job_Type con One Hot Encoding ignoramos las correlaciones. No existen muchas otras correlaciones notables.

## Machine Learning

### Selección de atributos / Feature Selection
Identificamos y seleccinoamos los atributos más relevantes. Utilizaremos un método de ANOVA (Analysis of Variance) ya que funciona bien con variables numéricas y categóricas al mismo tiempo.
"""

X = df.drop('Classification', axis = 1)
y = df['Classification']

fs = SelectKBest(score_func=f_regression,k=17)

fit = fs.fit(X,y)

"""Visualizamos los features ordenados por F-Score."""

features_score = pd.DataFrame(fit.scores_)
features = pd.DataFrame(X.columns)
feature_score = pd.concat([features,features_score],axis=1)

feature_score.columns = ["Input_Features","F_Score"]
print(feature_score.nlargest(15,columns="F_Score"))

"""Eliminaremos las columnas de Job_Type. Tienen poco valor y al ser el resultado de One Hot Encoding no permiten probar modelos basados en arboles."""

drop_cols = [
    'Job_Type_Unskilled - Resident',
    'Job_Type_Skilled Employee/Official',
    'Job_Type_Unemployed - Non-Resident',
    'Job_Type_Management/Self-Employed'
]

df.drop(drop_cols, axis = 1, inplace = True)

"""### Construcción del modelo / Model Building

#### Test-Train Split
Dividimos los datos en conjuntos de entrenamiento y prueba
"""

from sklearn.model_selection import train_test_split

X = df.drop('Classification', axis=1)
y = df['Classification']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#### Selección del modelo

Probaremos con algoritmos menos sensibles a imbalances en las clases ya que solo el 30% del dataset tiene clasificación de mal deudor. Utilizaremos:
* Random Forest Classifier
* Gradient Boosting Classifier (GBM)
* LightGBM
* XGBoost
* EasyEnsemble

Entrenamos los modelos base:
"""

models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'LightGBM': LGBMClassifier(random_state=42),
    'XGBoost': XGBClassifier(random_state=42),
    'EasyEnsemble': EasyEnsembleClassifier(random_state=42)
}

results = []

for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred)

    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

    npv = tn / (tn + fn)
    specificity = tn / (tn + fp)

    results.append({
        'Model': model_name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'ROC AUC': roc_auc,
        'NPV': npv, # Negative Predictive Value
        'Specificity': specificity
    })

"""Visualizamos los resultados."""

results_df = pd.DataFrame(results)
results_df

"""Visualizaremos las métricas del rendimiento de los modelos."""

import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid")

metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']

melted_df = results_df.melt(id_vars='Model', value_vars=metrics_to_plot, var_name='Metric')

plt.figure(figsize=(12, 6))
sns.barplot(x='Model', y='value', hue='Metric', data=melted_df)
plt.title('Métricas del Rendimiento de los Modelos')
plt.xlabel('Modelo')
plt.ylabel('Valor')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Métricas', loc='upper left', bbox_to_anchor=(1, 1))

plt.tight_layout()
plt.show()

"""Dado que es mejor optimizar para identificar correctamente los malos deudores, optimizaremos para el valor predictivo negativo y accuracy.Tomando esto en cuenta el mejor modelo es Gradient Boosting. Lo afinaremos a continuación.

#### Afinación de hiperparámetros
"""

gb_classifier = GradientBoostingClassifier(random_state=42)

# Define hyperparameter grid for Grid Search with the top 6 hyperparameters
param_grid = {
    'n_estimators': [100, 200, 500],
    'learning_rate': [0.01, 0.05, 0.08],
    'max_depth': [5, 7, 10],
    'min_samples_split': [2, 4, 6],
    'min_samples_leaf': [1, 2, 3],
    'subsample': [0.7, 0.8, 1.0],
}

# Create the Grid Search object
grid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# Fit the Grid Search to the training data
grid_search.fit(X_train, y_train)

# Get the best hyperparameters from the Grid Search
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Evaluate the model with the best hyperparameters on the test set
best_gb_classifier = grid_search.best_estimator_
accuracy = best_gb_classifier.score(X_test, y_test)
print("Test Accuracy with Best Hyperparameters:", accuracy)

"""#### Evaluación del modelo

##### Preparar k-Fold cross validation

Evaluaremos cada modelo usando el k-Fold cross validation. El k-Fold cross validation parte los datos en una cantidad _k_ de subconjuntos y evalúa al modelo entrenandolo con una cantidad _k-1_ de subconjuntos y se evalúa con el último subconjunto (Fold en inglés, de ahí el nombre). El proceso se repite _k_ veces, cada subcojunto tiene la oportunidad de ser el de prueba una vez.

Importar librería para realiar k-fold cross validation en todos nuestros modelos.
"""

from sklearn.model_selection import KFold
import statistics

"""Crear estructura de datos para almacenar las métricas."""

validation_results = {}

"""Matriz de confusión

#### Afinación de hiperparámetros / Hyperparameter tuning
"""



"""### Resultados

## Conclusión
"""